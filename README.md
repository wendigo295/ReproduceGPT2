I built and trained the GPT-2 model from scratch in Pytorch watching **Andrez Karpathy's** Video "**Let's Reproduce GPT-2**". I understood and wrote the code from scratch trained the model on the **FineWeb(EDU)** dataset on a batch size of **0.5M** tokens and evaluated it by the **HellaSwag** dataset which is also used in the GPT3 paper on 4 epochs. The trained model **surpassed** the original **GPT-2(124M)** model and almost reached the accuracy of **GPT-3** in evaluation.

Note that GPT-2 and GPT-3 are both simple language models, trained on internet documents, and all they do is "dream" internet documents. So this repo/video this does not cover Chat finetuning, and you can't talk to it like you can talk to ChatGPT. The finetuning process (while quite simple conceptually - SFT is just about swapping out the dataset and continuing the training) comes after this part and will be covered at a later time. For now this is the kind of stuff that the 124M model says if you prompt it with "Hello, I'm a language model," after 10B tokens of training:

```
Hello, I'm a language model, and my goal is to make English as easy and fun as possible for everyone, and to find out the different grammar rules
Hello, I'm a language model, so the next time I go, I'll just say, I like this stuff.
Hello, I'm a language model, and the question is, what should I do if I want to be a teacher?
Hello, I'm a language model, and I'm an English person. In languages, "speak" is really speaking. Because for most people, there's
```

And after 40B tokens of training:

```
Hello, I'm a language model, a model of computer science, and it's a way (in mathematics) to program computer programs to do things like write
Hello, I'm a language model, not a human. This means that I believe in my language model, as I have no experience with it yet.
Hello, I'm a language model, but I'm talking about data. You've got to create an array of data: you've got to create that.
Hello, I'm a language model, and all of this is about modeling and learning Python. I'm very good in syntax, however I struggle with Python due

```
ğŠğğ² ğğ¨ğ¢ğ§ğ­ğ¬:

ğğğ­ğ°ğ¨ğ«ğ¤ ğ’ğ¤ğğ¥ğğ­ğ¨ğ§ ğ‚ğ¨ğ§ğ¬ğ­ğ«ğ®ğœğ­ğ¢ğ¨ğ§: Constructed the entire network skeleton to match the schema of GPT-2 as used in Hugging Face transformers.

ğ’ğ¡ğšğ«ğğ ğ–ğğ¢ğ ğ¡ğ­ ğŒğšğ­ğ«ğ¢ğ±: Shared the same weight matrix between the embedding layers and the pre-softmax linear transformation, as specified in the GPT-2 paper.

ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğğ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğ°ğ¢ğ­ğ¡ ğ“ğ…ğŸ‘ğŸ: Training is optimized by setting matrix multiplication to the TF32 datatype, which offers the range of FP32 with the precision of FP16, resulting in an 8x speedup.

ğ€ğ®ğ­ğ¨ğ¦ğšğ­ğ¢ğœ ğŒğ¢ğ±ğğ ğğ«ğğœğ¢ğ¬ğ¢ğ¨ğ§: Further optimization is achieved using automatic mixed precision with ğ­ğ¨ğ«ğœğ¡.ğšğ®ğ­ğ¨ğœğšğ¬ğ­, improving efficiency to some extent.

ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ  ğğ©ğ­ğ¢ğ¦ğ¢ğ³ğšğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğ­ğ¨ğ«ğœğ¡.ğœğ¨ğ¦ğ©ğ¢ğ¥ğ: Training is additionally optimized by using ğ­ğ¨ğ«ğœğ¡.ğœğ¨ğ¦ğ©ğ¢ğ¥ğ, providing a 2x speedup. Karpathy explains this process via the Memory Hierarchy, focusing on Bandwidth and Memory Size.

ğ…ğ¥ğšğ¬ğ¡ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§ ğ“ğğœğ¡ğ§ğ¢ğªğ®ğ: Employed the Flash Attention technique, which avoids materializing large attention matrices and reads and writes to slow GPU HBM, resulting in a 7.6x speedup in attention computation.

ğ†ğ«ğšğğ¢ğğ§ğ­ ğ€ğœğœğ®ğ¦ğ®ğ¥ğšğ­ğ¢ğ¨ğ§ ğ“ğğœğ¡ğ§ğ¢ğªğ®ğ: Implemented gradient accumulation technique, allowing for training with huge batch sizes (up to 0.5M tokens) without overwhelming the GPU.

ğƒğ¢ğ¬ğ­ğ«ğ¢ğ›ğ®ğ­ğğ ğƒğšğ­ğš-ğğšğ«ğšğ¥ğ¥ğğ¥ ğ“ğ«ğšğ¢ğ§ğ¢ğ§ğ : Finally used Distributed Data-Parallel, making the necessary code adjustments and providing thorough explanations.

ğ…ğ¢ğ§ğğ–ğğ› ğƒğšğ­ğšğ¬ğğ­: Used the FineWeb(EDU) dataset, a filtered version of Common Crawl tailored to high-quality educational subsets for training.

ğ„ğ¯ğšğ¥ğ®ğšğ­ğ¢ğ¨ğ§ ğ°ğ¢ğ­ğ¡ ğ‡ğğ¥ğ¥ğšğ’ğ°ğšğ : For evaluation, Used HellaSwag, as utilized in the GPT-3 paper, surpassing GPT-2 and achieving almost GPT-3 level accuracy in just 4 epochs.

## License

MIT
